{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \\[1\\]:\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    tf.__version__\n",
    "\n",
    "Out\\[2\\]:\n",
    "\n",
    "    '2.9.1'\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                       shear_range = 0.2,\n",
    "                                       zoom_range = 0.2,\n",
    "                                       horizontal_flip = True)\n",
    "    training_set = train_datagen.flow_from_directory('D:\\\\DLNLP &MLBF\\\\CNN Dataset\\\\training_set',\n",
    "                                                     target_size = (64, 64),\n",
    "                                                     batch_size = 32,\n",
    "                                                     class_mode = 'binary')\n",
    "\n",
    "    Found 8000 images belonging to 2 classes.\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "    test_set = test_datagen.flow_from_directory('D:\\\\DLNLP &MLBF\\\\CNN Dataset\\\\test_set',\n",
    "                                                target_size = (64, 64),\n",
    "                                                batch_size = 32,\n",
    "                                                class_mode = 'binary')\n",
    "\n",
    "    Found 2000 images belonging to 2 classes.\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    cnn = tf.keras.models.Sequential()\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
    "\n",
    "    Epoch 1/25\n",
    "    250/250 [==============================] - 70s 261ms/step - loss: 0.6705 - accuracy: 0.5886 - val_loss: 0.6147 - val_accuracy: 0.6650\n",
    "    Epoch 2/25\n",
    "    250/250 [==============================] - 47s 189ms/step - loss: 0.6058 - accuracy: 0.6729 - val_loss: 0.5681 - val_accuracy: 0.7110\n",
    "    Epoch 3/25\n",
    "    250/250 [==============================] - 48s 191ms/step - loss: 0.5657 - accuracy: 0.7113 - val_loss: 0.5316 - val_accuracy: 0.7400\n",
    "    Epoch 4/25\n",
    "    250/250 [==============================] - 46s 183ms/step - loss: 0.5319 - accuracy: 0.7345 - val_loss: 0.5126 - val_accuracy: 0.7550\n",
    "    Epoch 5/25\n",
    "    250/250 [==============================] - 46s 183ms/step - loss: 0.5228 - accuracy: 0.7358 - val_loss: 0.4990 - val_accuracy: 0.7635\n",
    "    Epoch 6/25\n",
    "    250/250 [==============================] - 46s 184ms/step - loss: 0.5034 - accuracy: 0.7555 - val_loss: 0.4790 - val_accuracy: 0.7705\n",
    "    Epoch 7/25\n",
    "    250/250 [==============================] - 46s 185ms/step - loss: 0.4879 - accuracy: 0.7598 - val_loss: 0.5135 - val_accuracy: 0.7455\n",
    "    Epoch 8/25\n",
    "    250/250 [==============================] - 45s 180ms/step - loss: 0.4816 - accuracy: 0.7671 - val_loss: 0.4751 - val_accuracy: 0.7700\n",
    "    Epoch 9/25\n",
    "    250/250 [==============================] - 47s 189ms/step - loss: 0.4657 - accuracy: 0.7732 - val_loss: 0.4719 - val_accuracy: 0.7640\n",
    "    Epoch 10/25\n",
    "    250/250 [==============================] - 47s 189ms/step - loss: 0.4524 - accuracy: 0.7872 - val_loss: 0.4800 - val_accuracy: 0.7665\n",
    "    Epoch 11/25\n",
    "    250/250 [==============================] - 48s 191ms/step - loss: 0.4445 - accuracy: 0.7870 - val_loss: 0.4691 - val_accuracy: 0.7735\n",
    "    Epoch 12/25\n",
    "    250/250 [==============================] - 49s 195ms/step - loss: 0.4414 - accuracy: 0.7896 - val_loss: 0.5223 - val_accuracy: 0.7395\n",
    "    Epoch 13/25\n",
    "    250/250 [==============================] - 49s 195ms/step - loss: 0.4214 - accuracy: 0.8054 - val_loss: 0.5144 - val_accuracy: 0.7535\n",
    "    Epoch 14/25\n",
    "    250/250 [==============================] - 48s 194ms/step - loss: 0.4186 - accuracy: 0.8125 - val_loss: 0.4592 - val_accuracy: 0.7775\n",
    "    Epoch 15/25\n",
    "    250/250 [==============================] - 49s 197ms/step - loss: 0.4085 - accuracy: 0.8089 - val_loss: 0.4706 - val_accuracy: 0.7825\n",
    "    Epoch 16/25\n",
    "    250/250 [==============================] - 48s 192ms/step - loss: 0.4062 - accuracy: 0.8166 - val_loss: 0.4465 - val_accuracy: 0.7990\n",
    "    Epoch 17/25\n",
    "    250/250 [==============================] - 49s 196ms/step - loss: 0.3932 - accuracy: 0.8217 - val_loss: 0.4423 - val_accuracy: 0.7995\n",
    "    Epoch 18/25\n",
    "    250/250 [==============================] - 49s 194ms/step - loss: 0.3930 - accuracy: 0.8249 - val_loss: 0.4467 - val_accuracy: 0.7935\n",
    "    Epoch 19/25\n",
    "    250/250 [==============================] - 48s 192ms/step - loss: 0.3790 - accuracy: 0.8256 - val_loss: 0.4323 - val_accuracy: 0.8070\n",
    "    Epoch 20/25\n",
    "    250/250 [==============================] - 48s 191ms/step - loss: 0.3777 - accuracy: 0.8253 - val_loss: 0.4361 - val_accuracy: 0.8065\n",
    "    Epoch 21/25\n",
    "    250/250 [==============================] - 49s 196ms/step - loss: 0.3646 - accuracy: 0.8357 - val_loss: 0.4409 - val_accuracy: 0.7985\n",
    "    Epoch 22/25\n",
    "    250/250 [==============================] - 53s 214ms/step - loss: 0.3655 - accuracy: 0.8326 - val_loss: 0.4404 - val_accuracy: 0.7955\n",
    "    Epoch 23/25\n",
    "    250/250 [==============================] - 56s 223ms/step - loss: 0.3656 - accuracy: 0.8338 - val_loss: 0.4236 - val_accuracy: 0.8115\n",
    "    Epoch 24/25\n",
    "    250/250 [==============================] - 54s 215ms/step - loss: 0.3485 - accuracy: 0.8428 - val_loss: 0.4619 - val_accuracy: 0.8010\n",
    "    Epoch 25/25\n",
    "    250/250 [==============================] - 56s 223ms/step - loss: 0.3464 - accuracy: 0.8420 - val_loss: 0.4338 - val_accuracy: 0.8140\n",
    "\n",
    "Out\\[14\\]:\n",
    "\n",
    "    <keras.callbacks.History at 0x2704d3e14f0>\n",
    "\n",
    "In \\[30\\]:\n",
    "\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    test_image = image.load_img('D:\\\\DLNLP &MLBF\\\\CNN Dataset\\\\single_prediction\\\\cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = cnn.predict(test_image)\n",
    "    training_set.class_indices\n",
    "    if result[0][0] == 1:\n",
    "      prediction = 'dog'\n",
    "    else:\n",
    "      prediction = 'cat'\n",
    "\n",
    "    1/1 [==============================] - 6s 6s/step\n",
    "\n",
    "In \\[31\\]:\n",
    "\n",
    "    print(prediction)\n",
    "\n",
    "    cat\n",
    "\n",
    "In \\[ \\]:"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
